{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - Nearest Neighbors and Decision Trees\n",
    "\n",
    "## Lab objectives\n",
    "\n",
    "* Classification with decision trees and random forests.\n",
    "* Cross-validation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading training data\n",
      "Pre-loading test data\n"
     ]
    }
   ],
   "source": [
    "from lab_tools import CIFAR10, get_hog_image\n",
    "\n",
    "dataset = CIFAR10('./CIFAR10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nearest Neighbor\n",
    "\n",
    "The following example uses the Nearest Neighbor algorithm on the Histogram of Gradient decriptors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit( dataset.train['hog'], dataset.train['labels'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the **descriptive performance** of this classifier ?\n",
    "* Modify the code to estimate the **predictive performance**.\n",
    "* Use cross-validation to find the best hyper-parameters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 0 0 1]\n",
      "[1 2 2 ... 0 0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = clf.predict(dataset.train[\"hog\"])\n",
    "print(pred)\n",
    "print(dataset.train[\"labels\"])\n",
    "\n",
    "score = accuracy_score(dataset.train['labels'], pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13000  2487 10709 ... 14025  4367  5120]\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "proportion_validation = 0.2\n",
    "idxs = np.arange(len(dataset.train[\"labels\"]))\n",
    "np.random.shuffle(idxs)\n",
    "print(idxs)\n",
    "n_validation = int(proportion_validation * len(dataset.train[\"labels\"]))\n",
    "print(n_validation)\n",
    "validation_idxs = idxs[:n_validation]\n",
    "train_idxs = idxs[n_validation:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 256) (3000, 256)\n"
     ]
    }
   ],
   "source": [
    "train_X = dataset.train[\"hog\"][train_idxs]\n",
    "train_Y = dataset.train[\"labels\"][train_idxs]\n",
    "\n",
    "val_X = dataset.train[\"hog\"][validation_idxs]\n",
    "val_Y = dataset.train[\"labels\"][validation_idxs]\n",
    "\n",
    "print(train_X.shape, val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit( train_X, train_Y)\n",
    "pred = clf.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701\n",
      "[[631 226 144]\n",
      " [ 83 744 157]\n",
      " [ 31 256 728]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(val_Y, pred)\n",
    "score = accuracy_score(val_Y, pred)\n",
    "\n",
    "print(score)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors=1; pas de preproc; weight uniform; dist eucl. On voit donc que le pouvoir prédictif est de 70% et le pouvoir descriptif est de 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ks = [7]\n",
    "\n",
    "scores=[]\n",
    "for k in ks:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores += [cross_val_score(clf, dataset.train[\"hog\"], dataset.train[\"labels\"], cv=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70793333]\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores)\n",
    "print(scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors= 1, 3, 5, 7: résultats:0.6878; 0.70606667; 0.70973333; 0.70793333. avec cv=5.\n",
    "Ce paramètre est donc optimisé à 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) distance/metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "scores=[]\n",
    "for metric in metrics:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores += [cross_val_score(clf, dataset.train[\"hog\"], dataset.train[\"labels\"], cv=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "print(scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees\n",
    "\n",
    "[Decision Trees](http://scikit-learn.org/stable/modules/tree.html#tree) classify the data by splitting the feature space according to simple, single-feature rules. Scikit-learn uses the [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) algorithm for [its implementation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) of the classifier. \n",
    "\n",
    "* **Create a simple Decision Tree classifier** using scikit-learn and train it on the HoG training set.\n",
    "* Use cross-validation to find the best hyper-paramters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# --- Your code here --- #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forests\n",
    "\n",
    "[Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers use multiple decision trees trained on \"weaker\" datasets (less data and/or less features), averaging the results so as to reduce over-fitting.\n",
    "\n",
    "* Use scikit-learn to **create a Random Forest classifier** on the CIFAR data. \n",
    "* Use cross-validation to find the best hyper-paramters for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# --- Your code here --- #\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
